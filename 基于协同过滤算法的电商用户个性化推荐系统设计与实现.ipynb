{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0042608c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m coo_matrix, csr_matrix\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TruncatedSVD\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from textblob import TextBlob\n",
    "import stellargraph as sg\n",
    "from stellargraph.mapper import FullBatchNodeGenerator\n",
    "from stellargraph.layer import GCN\n",
    "from tensorflow.keras import Model\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "import os\n",
    "\n",
    "# 下载NLTK停用词\n",
    "nltk.download('stopwords')\n",
    "\n",
    "class RecommendationSystem:\n",
    "    def __init__(self):\n",
    "        # 配置参数\n",
    "        self.config = {\n",
    "            'time_decay_lambda': 0.2,\n",
    "            'min_user_interactions': 5,\n",
    "            'min_item_interactions': 10,\n",
    "            'top_k': 10,\n",
    "            'svd_factors': 50,\n",
    "            'gcn_embedding_size': 128,\n",
    "            'test_size': 0.2,\n",
    "            'random_state': 42,\n",
    "            'tfidf_max_features': 500\n",
    "        }\n",
    "        \n",
    "        # 数据存储\n",
    "        self.df = None\n",
    "        self.train_data = None\n",
    "        self.test_data = None\n",
    "        \n",
    "        # 模型存储\n",
    "        self.user_cf_model = None\n",
    "        self.item_cf_model = None\n",
    "        self.svdpp_model = None\n",
    "        self.hybrid_model = None\n",
    "        \n",
    "        # 结果存储\n",
    "        self.results = {}\n",
    "        \n",
    "        # 创建结果目录\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "        os.makedirs('visualizations', exist_ok=True)\n",
    "\n",
    "    def load_data(self, movielens_path, amazon_path):\n",
    "        \"\"\"加载并合并MovieLens和Amazon数据集\"\"\"\n",
    "        print(\"Loading and merging datasets...\")\n",
    "        \n",
    "        # 加载MovieLens数据\n",
    "        movie_data = pd.read_csv(movielens_path)\n",
    "        \n",
    "        # 加载Amazon数据\n",
    "        amazon_data = pd.read_csv(amazon_path)\n",
    "        \n",
    "        # 创建统一评分体系\n",
    "        combined_data = pd.concat([\n",
    "            movie_data[['userId', 'movieId', 'rating', 'timestamp']].rename(\n",
    "                columns={'movieId': 'itemId'}\n",
    "            ),\n",
    "            amazon_data[['reviewerID', 'asin', 'overall', 'reviewText', 'unixReviewTime']].rename(\n",
    "                columns={'reviewerID': 'userId', 'asin': 'itemId', 'overall': 'rating', \n",
    "                         'unixReviewTime': 'timestamp'}\n",
    "            )\n",
    "        ], ignore_index=True)\n",
    "        \n",
    "        # 转换时间戳\n",
    "        combined_data['timestamp'] = pd.to_datetime(combined_data['timestamp'], unit='s')\n",
    "        \n",
    "        # 填充缺失的reviewText\n",
    "        combined_data['reviewText'] = combined_data['reviewText'].fillna('')\n",
    "        \n",
    "        self.df = combined_data\n",
    "        print(f\"Loaded {len(self.df)} records with {self.df['userId'].nunique()} users and {self.df['itemId'].nunique()} items\")\n",
    "        \n",
    "        return self.df\n",
    "\n",
    "    def detect_anomalies(self):\n",
    "        \"\"\"检测并处理异常数据\"\"\"\n",
    "        print(\"Detecting anomalies...\")\n",
    "        \n",
    "        # 检测同一用户1分钟内产生超过10条记录\n",
    "        df_sorted = self.df.sort_values(['userId', 'timestamp'])\n",
    "        df_sorted['time_diff'] = df_sorted.groupby('userId')['timestamp'].diff().dt.total_seconds().fillna(0)\n",
    "        \n",
    "        # 标记异常用户\n",
    "        anomaly_users = set()\n",
    "        for user, group in df_sorted.groupby('userId'):\n",
    "            # 计算滚动窗口内的交互次数\n",
    "            rolling_count = group['time_diff'].rolling(window=10, min_periods=1).apply(\n",
    "                lambda x: np.sum(x < 60), raw=False\n",
    "            )\n",
    "            if any(rolling_count > 10):\n",
    "                anomaly_users.add(user)\n",
    "        \n",
    "        # 删除异常用户的所有记录\n",
    "        original_count = len(self.df)\n",
    "        self.df = self.df[~self.df['userId'].isin(anomaly_users)]\n",
    "        removed_count = original_count - len(self.df)\n",
    "        \n",
    "        print(f\"Removed {removed_count} records from {len(anomaly_users)} anomaly users\")\n",
    "        \n",
    "        return self.df\n",
    "\n",
    "    def normalize_ratings(self):\n",
    "        \"\"\"标准化评分到[0,1]区间\"\"\"\n",
    "        print(\"Normalizing ratings...\")\n",
    "        \n",
    "        # MovieLens评分标准化 (1-5 -> 0-1)\n",
    "        movielens_mask = self.df['itemId'].str.startswith('m')  # 假设MovieLens ID以'm'开头\n",
    "        self.df.loc[movielens_mask, 'rating_norm'] = (self.df.loc[movielens_mask, 'rating'] - 1) / 4\n",
    "        \n",
    "        # Amazon评分标准化 (1-5 -> 0.2-1.0)\n",
    "        amazon_mask = self.df['itemId'].str.startswith('B')  # 假设Amazon ASIN以'B'开头\n",
    "        self.df.loc[amazon_mask, 'rating_norm'] = self.df.loc[amazon_mask, 'rating'] / 5\n",
    "        \n",
    "        return self.df\n",
    "\n",
    "    def apply_time_decay(self):\n",
    "        \"\"\"应用时间衰减因子\"\"\"\n",
    "        print(\"Applying time decay...\")\n",
    "        \n",
    "        current_time = self.df['timestamp'].max()\n",
    "        time_diff = (current_time - self.df['timestamp']).dt.total_seconds() / 86400  # 转换为天\n",
    "        self.df['time_weight'] = np.exp(-self.config['time_decay_lambda'] * time_diff)\n",
    "        self.df['weighted_rating'] = self.df['rating_norm'] * self.df['time_weight']\n",
    "        \n",
    "        # 可视化时间衰减权重分布\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.kdeplot(self.df['time_weight'], fill=True)\n",
    "        plt.title('时间衰减权重分布', fontsize=14)\n",
    "        plt.xlabel('衰减权重', fontsize=12)\n",
    "        plt.ylabel('密度', fontsize=12)\n",
    "        plt.savefig('visualizations/time_weight_dist.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        return self.df\n",
    "\n",
    "    def extract_text_features(self):\n",
    "        \"\"\"提取文本特征和情感分析\"\"\"\n",
    "        print(\"Extracting text features...\")\n",
    "        \n",
    "        # 情感分析\n",
    "        self.df['sentiment'] = self.df['reviewText'].apply(\n",
    "            lambda x: TextBlob(str(x)).sentiment.polarity\n",
    "        )\n",
    "        \n",
    "        # TF-IDF特征提取\n",
    "        tfidf = TfidfVectorizer(\n",
    "            stop_words=stopwords.words('english'),\n",
    "            max_features=self.config['tfidf_max_features']\n",
    "        )\n",
    "        \n",
    "        # 只对Amazon评论应用TF-IDF\n",
    "        amazon_mask = self.df['itemId'].str.startswith('B')\n",
    "        text_features = tfidf.fit_transform(self.df.loc[amazon_mask, 'reviewText'])\n",
    "        \n",
    "        # 创建TF-IDF DataFrame\n",
    "        tfidf_df = pd.DataFrame(\n",
    "            text_features.toarray(), \n",
    "            columns=[f\"tfidf_{col}\" for col in tfidf.get_feature_names_out()],\n",
    "            index=self.df[amazon_mask].index\n",
    "        )\n",
    "        \n",
    "        # 合并回主数据框\n",
    "        self.df = pd.concat([self.df, tfidf_df], axis=1)\n",
    "        \n",
    "        # 填充NaN值为0\n",
    "        tfidf_cols = [col for col in self.df.columns if col.startswith('tfidf_')]\n",
    "        self.df[tfidf_cols] = self.df[tfidf_cols].fillna(0)\n",
    "        \n",
    "        return self.df\n",
    "\n",
    "    def filter_sparse_data(self):\n",
    "        \"\"\"过滤交互次数过少的用户和商品\"\"\"\n",
    "        print(\"Filtering sparse data...\")\n",
    "        \n",
    "        # 计算用户交互次数\n",
    "        user_counts = self.df['userId'].value_counts()\n",
    "        # 计算商品交互次数\n",
    "        item_counts = self.df['itemId'].value_counts()\n",
    "        \n",
    "        # 过滤交互次数过少的用户和商品\n",
    "        filtered_users = user_counts[user_counts >= self.config['min_user_interactions']].index\n",
    "        filtered_items = item_counts[item_counts >= self.config['min_item_interactions']].index\n",
    "        \n",
    "        original_size = len(self.df)\n",
    "        self.df = self.df[\n",
    "            self.df['userId'].isin(filtered_users) & \n",
    "            self.df['itemId'].isin(filtered_items)\n",
    "        ]\n",
    "        \n",
    "        print(f\"Filtered {original_size - len(self.df)} sparse records\")\n",
    "        print(f\"Remaining data: {len(self.df)} records, {self.df['userId'].nunique()} users, {self.df['itemId'].nunique()} items\")\n",
    "        \n",
    "        return self.df\n",
    "\n",
    "    def generate_graph_embeddings(self):\n",
    "        \"\"\"使用GCN生成图嵌入\"\"\"\n",
    "        print(\"Generating graph embeddings with GCN...\")\n",
    "        \n",
    "        # 创建StellarGraph对象\n",
    "        graph = sg.StellarDiGraph()\n",
    "        \n",
    "        # 添加节点\n",
    "        users = self.df['userId'].unique()\n",
    "        items = self.df['itemId'].unique()\n",
    "        \n",
    "        graph.add_nodes_from(users, node_type=\"user\")\n",
    "        graph.add_nodes_from(items, node_type=\"item\")\n",
    "        \n",
    "        # 添加边\n",
    "        edges = [(row['userId'], row['itemId']) for _, row in self.df.iterrows()]\n",
    "        graph.add_edges_from(edges, edge_type=\"interaction\")\n",
    "        \n",
    "        # 创建GCN模型\n",
    "        generator = FullBatchNodeGenerator(graph, method=\"gcn\")\n",
    "        gcn = GCN(\n",
    "            layer_sizes=[self.config['gcn_embedding_size']], \n",
    "            generator=generator,\n",
    "            activations=[\"relu\"]\n",
    "        )\n",
    "        \n",
    "        # 构建模型\n",
    "        x_in, x_out = gcn.in_out_tensors()\n",
    "        model = Model(inputs=x_in, outputs=x_out)\n",
    "        \n",
    "        # 训练模型（这里简化了训练过程）\n",
    "        # 在实际应用中，需要更复杂的训练过程\n",
    "        embeddings = model.predict(generator.flow(graph.nodes()))\n",
    "        \n",
    "        # 提取用户嵌入\n",
    "        user_embeddings = embeddings[0][:len(users)]\n",
    "        user_embedding_df = pd.DataFrame(\n",
    "            user_embeddings,\n",
    "            index=users,\n",
    "            columns=[f\"gcn_embed_{i}\" for i in range(self.config['gcn_embedding_size'])]\n",
    "        )\n",
    "        \n",
    "        # 提取商品嵌入\n",
    "        item_embeddings = embeddings[0][len(users):]\n",
    "        item_embedding_df = pd.DataFrame(\n",
    "            item_embeddings,\n",
    "            index=items,\n",
    "            columns=[f\"gcn_embed_{i}\" for i in range(self.config['gcn_embedding_size'])]\n",
    "        )\n",
    "        \n",
    "        # 合并嵌入到主数据框\n",
    "        self.df = self.df.merge(user_embedding_df, left_on='userId', right_index=True, how='left')\n",
    "        self.df = self.df.merge(item_embedding_df, left_on='itemId', right_index=True, how='left')\n",
    "        \n",
    "        # 填充NaN值为0\n",
    "        gcn_cols = [col for col in self.df.columns if col.startswith('gcn_embed_')]\n",
    "        self.df[gcn_cols] = self.df[gcn_cols].fillna(0)\n",
    "        \n",
    "        print(f\"Generated GCN embeddings with {self.config['gcn_embedding_size']} dimensions\")\n",
    "        \n",
    "        return self.df\n",
    "\n",
    "    def split_data(self):\n",
    "        \"\"\"划分训练集和测试集\"\"\"\n",
    "        print(\"Splitting data into train and test sets...\")\n",
    "        \n",
    "        # 按时间划分：最后20%时间的数据作为测试集\n",
    "        self.df = self.df.sort_values('timestamp')\n",
    "        split_idx = int(len(self.df) * (1 - self.config['test_size']))\n",
    "        \n",
    "        self.train_data = self.df.iloc[:split_idx]\n",
    "        self.test_data = self.df.iloc[split_idx:]\n",
    "        \n",
    "        print(f\"Train set: {len(self.train_data)} records\")\n",
    "        print(f\"Test set: {len(self.test_data)} records\")\n",
    "        \n",
    "        return self.train_data, self.test_data\n",
    "\n",
    "    def create_interaction_matrix(self, data):\n",
    "        \"\"\"创建用户-商品交互矩阵\"\"\"\n",
    "        # 创建用户和商品的映射\n",
    "        unique_users = data['userId'].unique()\n",
    "        unique_items = data['itemId'].unique()\n",
    "        \n",
    "        user_to_idx = {user: idx for idx, user in enumerate(unique_users)}\n",
    "        item_to_idx = {item: idx for idx, item in enumerate(unique_items)}\n",
    "        \n",
    "        # 创建交互矩阵\n",
    "        rows = data['userId'].map(user_to_idx)\n",
    "        cols = data['itemId'].map(item_to_idx)\n",
    "        values = data['weighted_rating']\n",
    "        \n",
    "        interaction_matrix = coo_matrix(\n",
    "            (values, (rows, cols)),\n",
    "            shape=(len(unique_users), len(unique_items))\n",
    "        ).tocsr()\n",
    "        \n",
    "        return interaction_matrix, user_to_idx, item_to_idx, unique_users, unique_items\n",
    "\n",
    "    class UserCF:\n",
    "        \"\"\"基于用户的协同过滤\"\"\"\n",
    "        def __init__(self, k=10, similarity_threshold=0.3):\n",
    "            self.k = k\n",
    "            self.similarity_threshold = similarity_threshold\n",
    "            self.interaction_matrix = None\n",
    "            self.user_to_idx = None\n",
    "            self.idx_to_user = None\n",
    "            self.item_to_idx = None\n",
    "            self.idx_to_item = None\n",
    "        \n",
    "        def fit(self, interaction_matrix, user_to_idx, item_to_idx):\n",
    "            self.interaction_matrix = interaction_matrix\n",
    "            self.user_to_idx = user_to_idx\n",
    "            self.idx_to_user = {v: k for k, v in user_to_idx.items()}\n",
    "            self.item_to_idx = item_to_idx\n",
    "            self.idx_to_item = {v: k for k, v in item_to_idx.items()}\n",
    "            \n",
    "            # 计算用户相似度矩阵\n",
    "            self.user_similarity = cosine_similarity(interaction_matrix)\n",
    "            \n",
    "            # 应用阈值\n",
    "            self.user_similarity[self.user_similarity < self.similarity_threshold] = 0\n",
    "        \n",
    "        def predict(self, user_id, n=10):\n",
    "            if user_id not in self.user_to_idx:\n",
    "                return []\n",
    "            \n",
    "            user_idx = self.user_to_idx[user_id]\n",
    "            \n",
    "            # 获取相似用户\n",
    "            similar_users = np.argsort(self.user_similarity[user_idx])[::-1][1:self.k+1]\n",
    "            \n",
    "            # 计算推荐分数\n",
    "            scores = np.zeros(self.interaction_matrix.shape[1])\n",
    "            \n",
    "            for sim_user_idx in similar_users:\n",
    "                similarity = self.user_similarity[user_idx, sim_user_idx]\n",
    "                if similarity > 0:\n",
    "                    scores += similarity * self.interaction_matrix[sim_user_idx].toarray().flatten()\n",
    "            \n",
    "            # 排除用户已经交互过的商品\n",
    "            interacted_items = self.interaction_matrix[user_idx].indices\n",
    "            scores[interacted_items] = -np.inf\n",
    "            \n",
    "            # 获取top N推荐\n",
    "            top_item_indices = np.argsort(scores)[::-1][:n]\n",
    "            return [self.idx_to_item[idx] for idx in top_item_indices if scores[idx] > 0]\n",
    "\n",
    "    class ItemCF:\n",
    "        \"\"\"基于物品的协同过滤\"\"\"\n",
    "        def __init__(self, k=10, similarity_threshold=0.3):\n",
    "            self.k = k\n",
    "            self.similarity_threshold = similarity_threshold\n",
    "            self.interaction_matrix = None\n",
    "            self.user_to_idx = None\n",
    "            self.idx_to_user = None\n",
    "            self.item_to_idx = None\n",
    "            self.idx_to_item = None\n",
    "        \n",
    "        def fit(self, interaction_matrix, user_to_idx, item_to_idx):\n",
    "            self.interaction_matrix = interaction_matrix\n",
    "            self.user_to_idx = user_to_idx\n",
    "            self.idx_to_user = {v: k for k, v in user_to_idx.items()}\n",
    "            self.item_to_idx = item_to_idx\n",
    "            self.idx_to_item = {v: k for k, v in item_to_idx.items()}\n",
    "            \n",
    "            # 计算物品相似度矩阵\n",
    "            self.item_similarity = cosine_similarity(interaction_matrix.T)\n",
    "            \n",
    "            # 应用阈值\n",
    "            self.item_similarity[self.item_similarity < self.similarity_threshold] = 0\n",
    "        \n",
    "        def predict(self, user_id, n=10):\n",
    "            if user_id not in self.user_to_idx:\n",
    "                return []\n",
    "            \n",
    "            user_idx = self.user_to_idx[user_id]\n",
    "            \n",
    "            # 获取用户交互过的商品\n",
    "            interacted_items = self.interaction_matrix[user_idx].indices\n",
    "            \n",
    "            # 计算推荐分数\n",
    "            scores = np.zeros(self.interaction_matrix.shape[1])\n",
    "            \n",
    "            for item_idx in interacted_items:\n",
    "                # 获取相似商品\n",
    "                similar_items = np.argsort(self.item_similarity[item_idx])[::-1][1:self.k+1]\n",
    "                \n",
    "                for sim_item_idx in similar_items:\n",
    "                    similarity = self.item_similarity[item_idx, sim_item_idx]\n",
    "                    if similarity > 0:\n",
    "                        scores[sim_item_idx] += similarity * self.interaction_matrix[user_idx, item_idx]\n",
    "            \n",
    "            # 排除用户已经交互过的商品\n",
    "            scores[interacted_items] = -np.inf\n",
    "            \n",
    "            # 获取top N推荐\n",
    "            top_item_indices = np.argsort(scores)[::-1][:n]\n",
    "            return [self.idx_to_item[idx] for idx in top_item_indices if scores[idx] > 0]\n",
    "\n",
    "    class SVDpp:\n",
    "        \"\"\"SVD++矩阵分解模型\"\"\"\n",
    "        def __init__(self, n_factors=50, n_epochs=20, lr=0.005, reg=0.02):\n",
    "            self.n_factors = n_factors\n",
    "            self.n_epochs = n_epochs\n",
    "            self.lr = lr\n",
    "            self.reg = reg\n",
    "            self.user_factors = None\n",
    "            self.item_factors = None\n",
    "            self.user_biases = None\n",
    "            self.item_biases = None\n",
    "            self.global_bias = None\n",
    "        \n",
    "        def fit(self, interaction_matrix):\n",
    "            # 转换为COO格式\n",
    "            coo_matrix = interaction_matrix.tocoo()\n",
    "            rows = coo_matrix.row\n",
    "            cols = coo_matrix.col\n",
    "            values = coo_matrix.data\n",
    "            \n",
    "            n_users, n_items = interaction_matrix.shape\n",
    "            \n",
    "            # 初始化参数\n",
    "            self.global_bias = np.mean(values)\n",
    "            self.user_biases = np.zeros(n_users)\n",
    "            self.item_biases = np.zeros(n_items)\n",
    "            self.user_factors = np.random.normal(0, 0.1, (n_users, self.n_factors))\n",
    "            self.item_factors = np.random.normal(0, 0.1, (n_items, self.n_factors))\n",
    "            \n",
    "            # 训练模型\n",
    "            for epoch in range(self.n_epochs):\n",
    "                for i, j, rating in zip(rows, cols, values):\n",
    "                    # 计算预测值\n",
    "                    pred = (\n",
    "                        self.global_bias + \n",
    "                        self.user_biases[i] + \n",
    "                        self.item_biases[j] + \n",
    "                        np.dot(self.user_factors[i], self.item_factors[j])\n",
    "                    )\n",
    "                    \n",
    "                    # 计算误差\n",
    "                    error = rating - pred\n",
    "                    \n",
    "                    # 更新参数\n",
    "                    self.user_biases[i] += self.lr * (error - self.reg * self.user_biases[i])\n",
    "                    self.item_biases[j] += self.lr * (error - self.reg * self.item_biases[j])\n",
    "                    \n",
    "                    # 更新因子\n",
    "                    uf = self.user_factors[i]\n",
    "                    itf = self.item_factors[j]\n",
    "                    \n",
    "                    self.user_factors[i] += self.lr * (error * itf - self.reg * uf)\n",
    "                    self.item_factors[j] += self.lr * (error * uf - self.reg * itf)\n",
    "            \n",
    "            return self\n",
    "        \n",
    "        def predict(self, user_idx, item_idx):\n",
    "            return (\n",
    "                self.global_bias + \n",
    "                self.user_biases[user_idx] + \n",
    "                self.item_biases[item_idx] + \n",
    "                np.dot(self.user_factors[user_idx], self.item_factors[item_idx])\n",
    "            )\n",
    "        \n",
    "        def recommend(self, user_idx, n=10, exclude_interacted=True):\n",
    "            # 计算所有商品的预测评分\n",
    "            scores = (\n",
    "                self.global_bias + \n",
    "                self.user_biases[user_idx] + \n",
    "                self.item_biases + \n",
    "                np.dot(self.user_factors[user_idx], self.item_factors.T)\n",
    "            )\n",
    "            \n",
    "            # 排除用户已经交互过的商品\n",
    "            if exclude_interacted:\n",
    "                interacted_items = self.interaction_matrix[user_idx].indices\n",
    "                scores[interacted_items] = -np.inf\n",
    "            \n",
    "            # 获取top N推荐\n",
    "            top_item_indices = np.argsort(scores)[::-1][:n]\n",
    "            return top_item_indices\n",
    "\n",
    "    class HybridModel:\n",
    "        \"\"\"混合推荐模型\"\"\"\n",
    "        def __init__(self, user_cf, item_cf, svdpp, weights=(0.4, 0.3, 0.3)):\n",
    "            self.user_cf = user_cf\n",
    "            self.item_cf = item_cf\n",
    "            self.svdpp = svdpp\n",
    "            self.weights = weights\n",
    "        \n",
    "        def predict(self, user_id, n=10):\n",
    "            # 获取各模型的推荐结果\n",
    "            user_cf_recs = self.user_cf.predict(user_id, n*3)\n",
    "            item_cf_recs = self.item_cf.predict(user_id, n*3)\n",
    "            svdpp_recs = self.svdpp.recommend(self.svdpp.user_to_idx[user_id], n*3)\n",
    "            \n",
    "            # 转换为商品ID\n",
    "            svdpp_recs = [self.svdpp.idx_to_item[idx] for idx in svdpp_recs]\n",
    "            \n",
    "            # 合并推荐结果并加权\n",
    "            combined_scores = {}\n",
    "            \n",
    "            # 为用户协同过滤结果加权\n",
    "            for i, item in enumerate(user_cf_recs):\n",
    "                combined_scores[item] = combined_scores.get(item, 0) + self.weights[0] * (1.0 / (i+1))\n",
    "            \n",
    "            # 为物品协同过滤结果加权\n",
    "            for i, item in enumerate(item_cf_recs):\n",
    "                combined_scores[item] = combined_scores.get(item, 0) + self.weights[1] * (1.0 / (i+1))\n",
    "            \n",
    "            # 为SVD++结果加权\n",
    "            for i, item in enumerate(svdpp_recs):\n",
    "                combined_scores[item] = combined_scores.get(item, 0) + self.weights[2] * (1.0 / (i+1))\n",
    "            \n",
    "            # 排序并获取top N\n",
    "            sorted_items = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "            return [item for item, score in sorted_items]\n",
    "\n",
    "    def train_models(self):\n",
    "        \"\"\"训练所有模型\"\"\"\n",
    "        print(\"Training models...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 创建交互矩阵\n",
    "        (train_matrix, user_to_idx, item_to_idx, \n",
    "         unique_users, unique_items) = self.create_interaction_matrix(self.train_data)\n",
    "        \n",
    "        # 训练User-CF模型\n",
    "        user_cf_start = time.time()\n",
    "        self.user_cf_model = self.UserCF(k=self.config['top_k'])\n",
    "        self.user_cf_model.fit(train_matrix, user_to_idx, item_to_idx)\n",
    "        user_cf_time = time.time() - user_cf_start\n",
    "        \n",
    "        # 训练Item-CF模型\n",
    "        item_cf_start = time.time()\n",
    "        self.item_cf_model = self.ItemCF(k=self.config['top_k'])\n",
    "        self.item_cf_model.fit(train_matrix, user_to_idx, item_to_idx)\n",
    "        item_cf_time = time.time() - item_cf_start\n",
    "        \n",
    "        # 训练SVD++模型\n",
    "        svdpp_start = time.time()\n",
    "        self.svdpp_model = self.SVDpp(n_factors=self.config['svd_factors'])\n",
    "        self.svdpp_model.interaction_matrix = train_matrix\n",
    "        self.svdpp_model.user_to_idx = user_to_idx\n",
    "        self.svdpp_model.idx_to_user = {v: k for k, v in user_to_idx.items()}\n",
    "        self.svdpp_model.item_to_idx = item_to_idx\n",
    "        self.svdpp_model.idx_to_item = {v: k for k, v in item_to_idx.items()}\n",
    "        self.svdpp_model.fit(train_matrix)\n",
    "        svdpp_time = time.time() - svdpp_start\n",
    "        \n",
    "        # 训练混合模型\n",
    "        hybrid_start = time.time()\n",
    "        self.hybrid_model = self.HybridModel(\n",
    "            self.user_cf_model, \n",
    "            self.item_cf_model, \n",
    "            self.svdpp_model\n",
    "        )\n",
    "        hybrid_time = time.time() - hybrid_start\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"Models trained in {total_time:.2f} seconds\")\n",
    "        print(f\"- User-CF: {user_cf_time:.2f}s\")\n",
    "        print(f\"- Item-CF: {item_cf_time:.2f}s\")\n",
    "        print(f\"- SVD++: {svdpp_time:.2f}s\")\n",
    "        print(f\"- Hybrid: {hybrid_time:.2f}s\")\n",
    "        \n",
    "        return {\n",
    "            'user_cf': user_cf_time,\n",
    "            'item_cf': item_cf_time,\n",
    "            'svdpp': svdpp_time,\n",
    "            'hybrid': hybrid_time,\n",
    "            'total': total_time\n",
    "        }\n",
    "\n",
    "    def evaluate_model(self, model, model_name):\n",
    "        \"\"\"评估模型性能\"\"\"\n",
    "        print(f\"Evaluating {model_name} model...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 准备测试数据\n",
    "        test_users = self.test_data['userId'].unique()\n",
    "        test_data_grouped = self.test_data.groupby('userId')['itemId'].apply(set).to_dict()\n",
    "        \n",
    "        # 评估指标\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        coverages = []\n",
    "        all_recommended = set()\n",
    "        all_items = set(self.df['itemId'].unique())\n",
    "        \n",
    "        # 长尾商品识别\n",
    "        item_popularity = self.df['itemId'].value_counts()\n",
    "        long_tail_items = set(item_popularity[item_popularity < 10].index)\n",
    "        \n",
    "        long_tail_ratios = []\n",
    "        novel_items = set()\n",
    "        \n",
    "        # 对每个测试用户进行评估\n",
    "        for i, user in enumerate(test_users):\n",
    "            if i > 0 and i % 100 == 0:\n",
    "                print(f\"Processed {i}/{len(test_users)} users...\")\n",
    "            \n",
    "            # 获取真实交互的商品\n",
    "            true_items = test_data_grouped.get(user, set())\n",
    "            \n",
    "            # 生成推荐\n",
    "            recommendations = model.predict(user, self.config['top_k'])\n",
    "            \n",
    "            # 计算Precision和Recall\n",
    "            if recommendations:\n",
    "                hits = len(set(recommendations) & true_items)\n",
    "                precisions.append(hits / len(recommendations))\n",
    "                recalls.append(hits / len(true_items) if true_items else 0)\n",
    "            else:\n",
    "                precisions.append(0)\n",
    "                recalls.append(0)\n",
    "            \n",
    "            # 计算覆盖率\n",
    "            all_recommended.update(recommendations)\n",
    "            coverages.append(len(all_recommended) / len(all_items))\n",
    "            \n",
    "            # 计算长尾商品比例\n",
    "            if recommendations:\n",
    "                long_tail_count = sum(1 for item in recommendations if item in long_tail_items)\n",
    "                long_tail_ratios.append(long_tail_count / len(recommendations))\n",
    "                novel_items.update([item for item in recommendations if item not in true_items])\n",
    "        \n",
    "        # 计算平均指标\n",
    "        precision_avg = np.mean(precisions)\n",
    "        recall_avg = np.mean(recalls)\n",
    "        coverage_avg = np.mean(coverages)\n",
    "        long_tail_avg = np.mean(long_tail_ratios) if long_tail_ratios else 0\n",
    "        novelty_avg = len(novel_items) / len(all_items) if all_items else 0\n",
    "        \n",
    "        eval_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"Evaluation completed in {eval_time:.2f} seconds\")\n",
    "        print(f\"- Precision@{self.config['top_k']}: {precision_avg:.4f}\")\n",
    "        print(f\"- Recall@{self.config['top_k']}: {recall_avg:.4f}\")\n",
    "        print(f\"- Coverage@{self.config['top_k']}: {coverage_avg:.4f}\")\n",
    "        print(f\"- Long Tail Ratio: {long_tail_avg:.4f}\")\n",
    "        print(f\"- Novelty: {novelty_avg:.4f}\")\n",
    "        \n",
    "        # 保存结果\n",
    "        self.results[model_name] = {\n",
    "            'precision': precision_avg,\n",
    "            'recall': recall_avg,\n",
    "            'coverage': coverage_avg,\n",
    "            'long_tail': long_tail_avg,\n",
    "            'novelty': novelty_avg,\n",
    "            'eval_time': eval_time\n",
    "        }\n",
    "        \n",
    "        return self.results[model_name]\n",
    "\n",
    "    def visualize_results(self):\n",
    "        \"\"\"可视化评估结果\"\"\"\n",
    "        print(\"Visualizing results...\")\n",
    "        \n",
    "        # 准备数据\n",
    "        models = list(self.results.keys())\n",
    "        metrics = ['precision', 'recall', 'coverage', 'long_tail']\n",
    "        metric_names = ['Precision@10', 'Recall@10', 'Coverage@10', 'Long Tail Ratio']\n",
    "        \n",
    "        # 创建子图\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        # 绘制每个指标的柱状图\n",
    "        for i, metric in enumerate(metrics):\n",
    "            values = [self.results[model][metric] for model in models]\n",
    "            ax = axes[i]\n",
    "            bars = ax.bar(models, values, color=sns.color_palette(\"viridis\", len(models)))\n",
    "            ax.set_title(metric_names[i], fontsize=14)\n",
    "            ax.set_ylabel('Score', fontsize=12)\n",
    "            ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "            \n",
    "            # 在柱子上方添加数值\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax.annotate(f'{height:.4f}',\n",
    "                            xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                            xytext=(0, 3),  # 3 points vertical offset\n",
    "                            textcoords=\"offset points\",\n",
    "                            ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('results/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # 训练和评估时间对比\n",
    "        time_data = {\n",
    "            'Training Time': {\n",
    "                'User-CF': self.results['User-CF'].get('train_time', 0),\n",
    "                'Item-CF': self.results['Item-CF'].get('train_time', 0),\n",
    "                'SVD++': self.results['SVD++'].get('train_time', 0),\n",
    "                'Hybrid': self.results['Hybrid'].get('train_time', 0)\n",
    "            },\n",
    "            'Evaluation Time': {\n",
    "                'User-CF': self.results['User-CF']['eval_time'],\n",
    "                'Item-CF': self.results['Item-CF']['eval_time'],\n",
    "                'SVD++': self.results['SVD++']['eval_time'],\n",
    "                'Hybrid': self.results['Hybrid']['eval_time']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        bar_width = 0.35\n",
    "        index = np.arange(len(models))\n",
    "        \n",
    "        train_times = [time_data['Training Time'][model] for model in models]\n",
    "        eval_times = [time_data['Evaluation Time'][model] for model in models]\n",
    "        \n",
    "        bar1 = ax.bar(index, train_times, bar_width, label='Training Time')\n",
    "        bar2 = ax.bar(index + bar_width, eval_times, bar_width, label='Evaluation Time')\n",
    "        \n",
    "        ax.set_xlabel('Model', fontsize=12)\n",
    "        ax.set_ylabel('Time (seconds)', fontsize=12)\n",
    "        ax.set_title('Model Training and Evaluation Time', fontsize=14)\n",
    "        ax.set_xticks(index + bar_width / 2)\n",
    "        ax.set_xticklabels(models)\n",
    "        ax.legend()\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('results/time_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # 用户行为时间模式可视化\n",
    "        self.df['hour'] = self.df['timestamp'].dt.hour\n",
    "        hourly_counts = self.df.groupby('hour').size()\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.lineplot(x=hourly_counts.index, y=hourly_counts.values)\n",
    "        plt.title('User Activity by Hour of Day', fontsize=14)\n",
    "        plt.xlabel('Hour of Day', fontsize=12)\n",
    "        plt.ylabel('Number of Interactions', fontsize=12)\n",
    "        plt.xticks(range(0, 24))\n",
    "        plt.grid(linestyle='--', alpha=0.7)\n",
    "        plt.savefig('results/hourly_activity.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # 长尾分布可视化\n",
    "        item_counts = self.df['itemId'].value_counts()\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(np.arange(len(item_counts)), item_counts.values)\n",
    "        plt.title('Item Popularity Distribution (Long Tail)', fontsize=14)\n",
    "        plt.xlabel('Item Rank', fontsize=12)\n",
    "        plt.ylabel('Interaction Count', fontsize=12)\n",
    "        plt.yscale('log')\n",
    "        plt.grid(linestyle='--', alpha=0.7)\n",
    "        plt.savefig('results/long_tail_distribution.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"Visualizations saved to results/ directory\")\n",
    "\n",
    "    def run_full_pipeline(self, movielens_path, amazon_path):\n",
    "        \"\"\"运行完整流程\"\"\"\n",
    "        # 1. 数据加载与预处理\n",
    "        self.load_data(movielens_path, amazon_path)\n",
    "        self.detect_anomalies()\n",
    "        self.normalize_ratings()\n",
    "        self.apply_time_decay()\n",
    "        self.extract_text_features()\n",
    "        self.filter_sparse_data()\n",
    "        \n",
    "        # 2. 特征工程\n",
    "        self.generate_graph_embeddings()\n",
    "        \n",
    "        # 3. 数据划分\n",
    "        self.split_data()\n",
    "        \n",
    "        # 4. 模型训练\n",
    "        train_times = self.train_models()\n",
    "        \n",
    "        # 5. 模型评估\n",
    "        self.evaluate_model(self.user_cf_model, \"User-CF\")\n",
    "        self.evaluate_model(self.item_cf_model, \"Item-CF\")\n",
    "        self.evaluate_model(self.svdpp_model, \"SVD++\")\n",
    "        self.evaluate_model(self.hybrid_model, \"Hybrid\")\n",
    "        \n",
    "        # 添加训练时间到结果\n",
    "        for model in self.results:\n",
    "            self.results[model]['train_time'] = train_times[model.lower()]\n",
    "        \n",
    "        # 6. 结果可视化\n",
    "        self.visualize_results()\n",
    "        \n",
    "        # 7. 保存结果\n",
    "        results_df = pd.DataFrame(self.results).T\n",
    "        results_df.to_csv('results/final_results.csv')\n",
    "        \n",
    "        print(\"Full pipeline completed!\")\n",
    "        return results_df\n",
    "\n",
    "\n",
    "# 示例用法\n",
    "if __name__ == \"__main__\":\n",
    "    # 初始化推荐系统\n",
    "    rec_sys = RecommendationSystem()\n",
    "    \n",
    "    # 设置数据集路径\n",
    "    movielens_path = \"data/raw/movielens_ratings.csv\"\n",
    "    amazon_path = \"data/raw/amazon_reviews.csv\"\n",
    "    \n",
    "    # 运行完整流程\n",
    "    results = rec_sys.run_full_pipeline(movielens_path, amazon_path)\n",
    "    \n",
    "    # 打印最终结果\n",
    "    print(\"\\nFinal Results:\")\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c402c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [15 lines of output]\n",
      "      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "      rather than 'sklearn' for pip commands.\n",
      "      \n",
      "      Here is how to fix this error in the main use cases:\n",
      "      - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "      - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "      - if the 'sklearn' package is used by one of your dependencies,\n",
      "        it would be great if you take some time to track which package uses\n",
      "        'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "      - as a last resort, set the environment variable\n",
      "        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "      \n",
      "      More information is available at\n",
      "      https://github.com/scikit-learn/sklearn-pypi-package\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n",
      "WARNING: You are using pip version 22.0.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the 'C:\\Program Files\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
